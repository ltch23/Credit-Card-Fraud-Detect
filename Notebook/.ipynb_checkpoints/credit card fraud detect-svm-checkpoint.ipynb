{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "11290628-b996-418e-a3d6-9d36447d5ade",
    "_kg_hide-input": false,
    "_uuid": "047250539ea0427dbc3f7dd46eb7e02e79e58bf4"
   },
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "be228bff-6554-4f52-94df-79894f2eb721",
    "_uuid": "764aeb6dd7615b00d5f050d7d6be68bd19199db1"
   },
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "\n",
    "dataset = pd.read_csv('../input/creditcard.csv')\n",
    "dataset = dataset.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)\n",
    "dataset['V1_'] = dataset.V1.map(lambda x: 1 if x < -3 else 0)\n",
    "dataset['V2_'] = dataset.V2.map(lambda x: 1 if x > 2.5 else 0)\n",
    "dataset['V3_'] = dataset.V3.map(lambda x: 1 if x < -4 else 0)\n",
    "dataset['V4_'] = dataset.V4.map(lambda x: 1 if x > 2.5 else 0)\n",
    "dataset['V5_'] = dataset.V5.map(lambda x: 1 if x < -4.5 else 0)\n",
    "dataset['V6_'] = dataset.V6.map(lambda x: 1 if x < -2.5 else 0)\n",
    "dataset['V7_'] = dataset.V7.map(lambda x: 1 if x < -3 else 0)\n",
    "dataset['V9_'] = dataset.V9.map(lambda x: 1 if x < -2 else 0)\n",
    "dataset['V10_'] = dataset.V10.map(lambda x: 1 if x < -2.5 else 0)\n",
    "dataset['V11_'] = dataset.V11.map(lambda x: 1 if x > 2 else 0)\n",
    "dataset['V12_'] = dataset.V12.map(lambda x: 1 if x < -2 else 0)\n",
    "dataset['V14_'] = dataset.V14.map(lambda x: 1 if x < -2.5 else 0)\n",
    "dataset['V16_'] = dataset.V16.map(lambda x: 1 if x < -2 else 0)\n",
    "dataset['V17_'] = dataset.V17.map(lambda x: 1 if x < -2 else 0)\n",
    "dataset['V18_'] = dataset.V18.map(lambda x: 1 if x < -2 else 0)\n",
    "dataset['V19_'] = dataset.V19.map(lambda x: 1 if x > 1.5 else 0)\n",
    "dataset['V21_'] = dataset.V21.map(lambda x: 1 if x > 0.6 else 0)\n",
    "\n",
    "#Crea una nueva función para transacciones normales (no fraudulentas).\n",
    "dataset.loc[dataset.Class == 0, 'Clase'] = 0\n",
    "dataset.loc[dataset.Class == 1, 'Clase'] = 1\n",
    "dataset = dataset.drop(['Class'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "976a06ca-b1d3-44b6-bc3e-8afd7160b63d",
    "_uuid": "7affa3554319cade77750157fd168813c53db17a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "# Parameters and results\n",
    "# Disminuya todas las características que tienen distribuciones muy similares entre los dos tipos de transacciones.\n",
    "print(dataset.shape[1])\n",
    "n_len=dataset.shape[1]\n",
    "pd.set_option(\"display.max_columns\",101)\n",
    "dataset.head()\n",
    "\n",
    "x = dataset.iloc[: , 1:n_len-1].values\n",
    "y = dataset.iloc[:, n_len-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "976a06ca-b1d3-44b6-bc3e-8afd7160b63d",
    "_uuid": "7affa3554319cade77750157fd168813c53db17a"
   },
   "outputs": [],
   "source": [
    "# Handling Missing Values\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "imputer = imputer.fit(x[:, 1::n_len-1])\n",
    "x[:, 1::n_len-1] = imputer.fit_transform(x[:, 1::n_len-1])\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "x = sc_x.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import KFold, cross_validate\n",
    "#scoring = ['accuracy', 'f1']\n",
    "#kfold = KFold(n_splits=3)\n",
    "#kfold = KFold(n_splits=6, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = cross_validate(rf_classifier,x,y,cv=kfold, scoring=scoring,return_train_score=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Random Forest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y,test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 'warn',\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier=RandomForestClassifier(random_state = 42)\n",
    "from pprint import pprint\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_classifier.get_params())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'criterion': ['gini', 'entropy'],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [50, 100, 150, 200]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#criterio\n",
    "criterion=['gini','entropy']\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 200, num = 4)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'criterion':criterion,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 52.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=-1,\n",
       "          param_distributions={'bootstrap': [True, False], 'min_samples_leaf': [1, 2, 4], 'n_estimators': [50, 100, 150, 200], 'min_samples_split': [2, 5, 10], 'criterion': ['gini', 'entropy'], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "          verbose=2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 75, scoring='neg_mean_absolute_error', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 150, 'max_features': 'auto', 'criterion': 'entropy', 'min_samples_split': 5, 'max_depth': 90}\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_params_)\n",
    "#print(rf_random.best_estimator_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
      "            criterion='entropy', max_depth=90, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=150, n_jobs=None, oob_score=False,\n",
      "            random_state=42, verbose=0, warm_start=False)\n",
      "[ 1.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "print(best_random)\n",
    "y_predict = best_random.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accura= accuracy_score(test_y, y_predict)  \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1score= f1_score(test_y, y_predict)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accura' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-86e531c72149>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccura\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"f1_score: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accura' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \",accura)\n",
    "print(\"f1_score: \",f1score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
